{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c078bd3-0660-487e-a31f-78807f859fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")\n",
    "load_dotenv(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a751673-c4bc-46c9-93e1-d27d600ae153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SACHENDRA\\miniconda3\\envs\\activeloop2\\Lib\\site-packages\\langchain\\output_parsers\\pydantic.py:20: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  \"\\{.*\\}\", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\activeloop2\\Lib\\site-packages\\langchain\\output_parsers\\pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     25\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str)\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\activeloop2\\Lib\\site-packages\\pydantic\\main.py:526\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\activeloop2\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mSuggestions)\n\u001b[0;32m     30\u001b[0m missformatted_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconduct\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanner\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefers to the way someone acts in a particular situation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefers to the way someone behaves in a particular situation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]}\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissformatted_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\activeloop2\\Lib\\site-packages\\langchain\\output_parsers\\pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     29\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     30\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from completion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The parsers are powerful tools to dynamically extract the information from the prompt and validate it to some extent.\n",
    "Still, they do not guarantee a response.\n",
    "Imagine a situation where you deployed your application, and the model’s response [to a user’s request] is incomplete, causing the parser to throw an error.\n",
    "It is not ideal! In the following subsections, we will introduce two classes acting as fail-safe.\n",
    "They add a layer on top of the model’s response to help fix the errors.\n",
    "\n",
    "NOTE: \n",
    "The following approaches work with the PydanticOutputParser class since it is the only one with a validation method.\n",
    "'''\n",
    "'''\n",
    "This method tries to fix the parsing error by looking at the model’s response and the previous parser.\n",
    "It uses a Large Language Model (LLM) to solve the issue.\n",
    "We will use GPT-3 to be consistent with the rest of the lesson, but it is possible to pass any supported model. \n",
    "Let’s start by defining the Pydantic data schema and show a sample error that could occur.\n",
    "'''\n",
    "\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
    "\n",
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'\n",
    "\n",
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b6176b-6206-4205-9bc5-2a6205454678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs you can see in the error message, the parser correctly identified an error in our sample response (missformatted_output) since we used the word reasoning instead of the expected reasons key. The OutputFixingParser class could easily fix this error.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "As you can see in the error message, the parser correctly identified an error in our sample response (missformatted_output) since we used the word reasoning instead of the expected reasons key. The OutputFixingParser class could easily fix this error.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05632db-4740-4983-8427-b169cd6581a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "model = OpenAI(model_name='gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026c8e2c-63b6-49f9-a1d4-6bb9d2e8a248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe from_llm() function takes the old parser and a language model as input parameters. Then, It initializes a new parser for you that has the ability to fix output errors. In this case, it successfully identified the misnamed key and changed it to what we defined.\\n\\nHowever, fixing the issues using this class is not always possible. Here is an example of using OutputFixingParser class to resolve an error with a missing key.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The from_llm() function takes the old parser and a language model as input parameters. Then, It initializes a new parser for you that has the ability to fix output errors. In this case, it successfully identified the misnamed key and changed it to what we defined.\n",
    "\n",
    "However, fixing the issues using this class is not always possible. Here is an example of using OutputFixingParser class to resolve an error with a missing key.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd9aea8-a589-4008-939b-2ac0fd6b09f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=['These words accurately describe the behavior in the given context.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'\n",
    "\n",
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "\n",
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ec294e-6406-46c2-982a-7e339d51b3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLooking at the output, it is evident that the model understood the key reasons missing from the response but didn’t have the context of the desired outcome. It created a list with one entry, while we expect one reason per word. This is why we sometimes need to use the RetryOutputParser class.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Looking at the output, it is evident that the model understood the key reasons missing from the response but didn’t have the context of the desired outcome. It created a list with one entry, while we expect one reason per word. This is why we sometimes need to use the RetryOutputParser class.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b09cd-2ae1-4c92-8891-a63f34112f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
