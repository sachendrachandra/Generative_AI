{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427c75a9-b947-4710-a80d-196dbf9e428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")\n",
    "load_dotenv(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7ba5b-b510-44fa-83c9-1b46aff0cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Large language models like GPT-3 and GPT-4 are pretrained on vast amounts of text data and learn to predict the next token in a sequence based \n",
    "on the context provided by the previous tokens.\n",
    "GPT-family models use Causal Language modeling, which predicts the next token while only having access to the tokens before it. \n",
    "This process enables LLMs to generate contextually relevant text.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ef7fc0-418a-49dd-a176-2dc7a43830d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainbow Threads\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8cf153-a8cd-4c6d-908f-17c357276a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTracking token Usage\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tracking token Usage\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bd4975-7ed1-41b1-9e85-9d7baa6da6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 263\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 259\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"davinci-002\", n=2, best_of=2)\n",
    "'''\n",
    "The callback will track the tokens used, successful requests, and total cost.\n",
    "'''\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f35ba2-5d77-4431-b7c4-88b796732920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
