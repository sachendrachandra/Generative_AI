{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b35e74-2943-4c20-8e05-1449d2d92d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "How GPT4All works?\n",
    "It is trained on top of Facebookâ€™s LLaMA model, which released its weights under a non-commercial license.\n",
    "Still, running the mentioned architecture on your local PC is impossible due to the large (7 billion) number of parameters. \n",
    "The authors incorporated two tricks to do efficient fine-tuning and inference. \n",
    "We will focus on inference since the fine-tuning process is out of the scope of this course.\n",
    "\n",
    "The main contribution of GPT4All models is the ability to run them on a CPU. \n",
    "Testing these models is practically free because the recent PCs have powerful Central Processing Units. \n",
    "The underlying algorithm that helps with making it happen is called Quantization.\n",
    "It basically converts the pre-trained model weights to 4-bit precision using the GGML format.\n",
    "So, the model uses fewer bits to represent the numbers. There are two main advantages to using this technique:\n",
    "\n",
    "1. Reducing Memory Usage: It makes deploying the models more efficient on low-resource devices.\n",
    "2. Faster Inference: The models will be faster during the generation process since there will be fewer computations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b441e9-0402-49e4-9912-eeab0d00b73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514266it [1:25:27, 100.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1:Convert The Model\n",
    "\n",
    "'''\n",
    "The first step is to download the weights and use a script from the LLaMAcpp repository to convert the weights from the old format to the new one.\n",
    "It is a required step; otherwise, the LangChain library will not identify the checkpoint file.\n",
    "\n",
    "We need to download the weights file. \n",
    "You can either head to [https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/] and download the weights (make sure to download the one that ends with *.ggml.bin) or \n",
    "use the following Python snippet that breaks down the file into multiple chunks and downloads them gradually. \n",
    "The local_path variable is the destination folder.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
    "\n",
    "# send a GET request to the URL to download the file.\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response\n",
    "# to it in chunks.\n",
    "with open(local_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa574138-3504-43ea-9ad9-62630bdbe5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "# from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc43183-0feb-430c-8cb5-6d9a225a5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe11787-e9ee-4906-8049-fc361d0ba6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Downloading gpt4all-2.6.0-py3-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from gpt4all) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from requests->gpt4all) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from requests->gpt4all) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from requests->gpt4all) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sachendra\\miniconda3\\envs\\activeloop2\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n",
      "Downloading gpt4all-2.6.0-py3-none-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.1/6.3 MB 1.2 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.2/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.2/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.2/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.3/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.4/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.5/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.0/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.1/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.1/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.2/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.2/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.3/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.4/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.4/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.5/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.6/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.7/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.9/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.9/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.0/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.1/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.2/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.2/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.3/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.4/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.5/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.5/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.7/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.7/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.8/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.9/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.9/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.0/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.0/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.1/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.2/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.2/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.3/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.3/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.4/6.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.5/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.5/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.6/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.6/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.8/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.8/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.1/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.1/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.2/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.3/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.3/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.4/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.6/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.8/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.8/6.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.9/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.1/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.2/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.3/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.3/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.4/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.6/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.7/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.7/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.9/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: gpt4all\n",
      "Successfully installed gpt4all-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39778d80-ff43-4fda-bf7d-9efbe4a418fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = AsyncCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model= \"all-MiniLM-L6-v2.gguf2.f16.gguf\", model_path=\"Documents/Activeloop/Langchain_and_Vector_Databases_in_Production/2_Large_Language_Models_and_Langchain/7_Using_OpenSource_GPT4All_Model_Locally/models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cfbd5-b3e3-410b-9439-f7029311e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
