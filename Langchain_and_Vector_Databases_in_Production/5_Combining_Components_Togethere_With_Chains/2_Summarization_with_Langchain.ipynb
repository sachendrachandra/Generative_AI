{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d079b6-15e7-43df-b2ac-daec8a9e3d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")\n",
    "load_dotenv(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd3fd23-22e3-4ee0-be40-07efd2461a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7baefc3-248f-469b-94f4-e882bce013c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55d880ff-d9c5-48fc-a933-5d5c7d7ff35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed613a77-9751-4cf4-aa09-608d8da9207d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Craig Smith interviews Jan LeCoon, a deep learning expert, about self-supervised learning and his\n",
      "new joint embedding predictive architecture. They discuss the limitations of large language models\n",
      "and the potential for AI systems to exhibit consciousness. Jan Le Ka, a professor at New York\n",
      "University and chief AI scientist at Fair, discusses the impact of self-supervised learning on\n",
      "natural language processing and the use of transformer architectures for pre-training. Self-\n",
      "supervised learning involves training large neural networks to predict missing words in text,\n",
      "leading to the development of good text representations that can be used for various downstream\n",
      "tasks. Generative models struggle to predict missing information accurately, posing challenges when\n",
      "applied to different types of data.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "# # Concatenate the text pieces into a single prompt\n",
    "# concatenated_prompt = \"\\n\\n\".join(texts)\n",
    "\n",
    "# # Generate a summary using the concatenated prompt\n",
    "# response = chain.apply(concatenated_prompt)\n",
    "\n",
    "# # Print the summary\n",
    "# wrapped_text = textwrap.fill(response, width=100)\n",
    "# print(wrapped_text)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89fcea2a-f6f7-4e67-abc3-5cb0987e064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "# With the following line of code, we can see the prompt template that is used with the map_reduce technique.\n",
    "# Now weâ€™re changing the prompt and using another summarization method:\n",
    "\n",
    "print( chain.llm_chain.prompt.template )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316cf54-6e14-435c-80dd-6f03bc8898be",
   "metadata": {},
   "source": [
    "The \"stuff\" approach is the simplest and most naive one, in which all the text from the transcribed video is used in a single prompt. This method may raise exceptions if all text is longer than the available context size of the LLM and may not be the most efficient way to handle large amounts of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cca5888f-f3b5-4b56-ab6e-4daece52ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36441c76-b720-4b2f-b56a-37f156185e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Craig Smith interviews Jan LeCoon, a key figure in deep learning and self-supervised learning\n",
      "- Jan discusses his joint embedding predictive architecture and its potential to fill gaps in large language models\n",
      "- Jan talks about the importance of self-supervised learning in natural language processing and its applications in various fields\n",
      "- Large language models lack a world model and struggle with uncertain predictions, especially in non-textual data like videos\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669a36c-d7f0-4073-b3a7-b5798eb48fb6",
   "metadata": {},
   "source": [
    "The 'refine' summarization chain is a method for generating more accurate and context-aware summaries. This chain type is designed to iteratively refine the summary by providing additional context when needed. That means: it generates the summary of the first chunk. Then, for each successive chunk, the work-in-progress summary is integrated with new info from the new chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73e0f731-4679-4402-9fd1-8acfaadcb1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Craig Smith interviews Jan LeCoon, a deep learning expert, about self-supervised learning and his\n",
      "new joint embedding predictive architecture. Jan, a professor at New York University and the chief\n",
      "AI scientist at Fair, discusses the revolution in natural language processing brought about by self-\n",
      "supervised learning and the use of transformer architectures for pre-training. He explains that\n",
      "self-supervised learning involves training large neural networks to predict missing words in text,\n",
      "leading to the development of effective text representations for various downstream tasks. Jan also\n",
      "mentions the widespread use of this technique in practical applications such as content moderation\n",
      "systems on platforms like Facebook, Google, and YouTube. Additionally, he touches on the limitations\n",
      "of large language models and his theory of consciousness in AI systems. The new context provided\n",
      "discusses the challenges of using generative models in self-supervised learning, particularly in\n",
      "handling uncertain predictions when applied to different types of data like video.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734c3bd-ff77-4787-91e9-f30050603781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
