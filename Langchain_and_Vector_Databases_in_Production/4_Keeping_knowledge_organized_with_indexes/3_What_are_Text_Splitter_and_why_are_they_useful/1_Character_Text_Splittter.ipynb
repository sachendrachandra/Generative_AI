{
 "cells": [
  {
   "cell_type": "raw",
   "id": "efa8a651-57c3-4e9a-88b5-e6afd3007456",
   "metadata": {},
   "source": [
    "Large Language Models, while recognized for creating human-like text, can also \"hallucinate\" and produce seemingly plausible yet incorrect or nonsensical information. Interestingly, this tendency can be advantageous in creative tasks, as it generates a range of unique and imaginative ideas, sparking new perspectives and driving the creative process. However, this poses a challenge in situations where accuracy is critical, such as code reviews, insurance-related tasks, or research question responses.\n",
    "\n",
    "One approach to mitigating hallucination is to provide documents as sources of information to the LLM and ask it to generate an answer based on the knowledge extracted from the document. This can help reduce the likelihood of hallucination, and users can verify the information with the source document.\n",
    "\n",
    "Let's discuss the pros and cons of this approach:\n",
    "\n",
    "Pros:\n",
    "\n",
    "Reduced hallucination: By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n",
    "Increased accuracy: With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n",
    "Verifiable information: Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\n",
    "Cons:\n",
    "\n",
    "Limited scope: Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n",
    "Dependence on document quality: The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n",
    "Inability to eliminate hallucination completely: Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information.\n",
    "Addressing another challenge, LLMs have a maximum prompt size, preventing them from feeding entire documents. This makes it crucial to divide documents into smaller parts, and Text Splitters prove to be extremely useful in achieving this. Text Splitters help break down large text documents into smaller, more digestible pieces that language models can process more effectively.\n",
    "\n",
    "Using a Text Splitter can also improve vector store search results, as smaller segments might be more likely to match a query. Experimenting with different chunk sizes and overlaps can be beneficial in tailoring results to suit your specific needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31a1a326-90b8-4833-b10a-732b22df2fb6",
   "metadata": {},
   "source": [
    "Character Text Splitter:\n",
    "This type of splitter can be used in various scenarios where you must split long text pieces into smaller, semantically meaningful chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84958ab3-6aa8-4b46-9201-f6095539c54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "config = dotenv_values(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")\n",
    "load_dotenv(\"C:/Users/SACHENDRA/Documents/Activeloop/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2b46ba-8621-4479-8c63-37a96bf25944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(r\"C:\\Users\\SACHENDRA\\Documents\\Activeloop\\Langchain_and_Vector_Databases_in_Production\\4_Keeping_knowledge_organized_with_indexes\\3_What_are_Text_Splitter_and_why_are_they_useful\\Sachendra_CV.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73f6cbb-0dfe-477a-a4d9-66fc18e77328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sachendra Chandra\\n♂phone6394633084, 9410411981 /envel⌢pe07sachendra@gmail.com /linkedinLinkedIn /githubGitHub\\nWork Experience\\nData Engineeer 2, Rakuten India August 2021 – Present\\nMart Migration\\n•Part of data platform migration project from Teradata data to Bigquery. Converted the Systemwalker Jobnet into\\nAirflow Python Scripts. These jobnets helped in creating the data marts in Teradata. The corresponding Airflow scripts\\norchestrated the same data mart creation in BigQuery.\\n•Analyzed the translated Teradata SQL to BigQuery SQLs and the data marts created in BigQuery to ensure exact match\\nbetween the data of Teradata that with BigQuery and automated the validation process for translated queries on Google\\nBigQuery and Hive and involed in data ingestion from Teradata to BigQuery via Jenkins.\\n•Part of migration task from C5000 to Minio and Bigquery. Translated the Airflow scripts for daily ingestion from Hive to\\nMinio and BigQuery.\\n•Skills Used: Python, SQL, Apache Airflow, BigQuery, Teradata, HiveQL\\nPOC of Data Mgmt. Tool like Spline and DataHub\\n•Integrated Spline Data Lineage tool with existing code base to track Lineage of Spark job used for mart creation in\\nBigQuery and Minio.\\n•Worked on creating a Custom dispatcher using the spark spline agent code base so as to extract relevant lineage\\ninformation for Spark mart creation jobs.\\n•Explored DataHub for capturing, tracking, and visualizing data lineage, allowing organizations to gain valuable insights\\ninto their data ecosystems.\\n•Skills Used: Python, Spline, DataHub, Apache Spark\\nInternal Metadata Management\\n•Worked on building framework to capture the dataset and datasource lineage and dataset metadata for spark job used\\nfor mart building.\\n•Developed REST Client SDK that leverages Hive Metastore service for capturing matadata information of spark job and\\ndispatching Metadata and lineage information to API layer.\\n•Skills Used: Apache Spark, Hive, ArangoDB, Docker\\nOperational Artificial General Intelligence (OAGI)\\n•Thousands of jobs running every day and they consume cores, memories, storages and network bandwidth to full-fill\\ntheir functional goals; those cores, memories, storages and network bandwidth are having the cost based on usage; if\\nthose processes are unnecessarily retried and rerun in a circumstance where errors are non recoverable, business has to\\npay unnecessary cost for those resources consumed by processes.\\n•Before retrying, the process asks for the opinion from the OAGI service which is powered by LLM(Large Language\\nModel) which has given the enough knowledge of what kind of exceptions/errors are re-triable and non re-triable. This\\nsaves many resources and directly impacts business revenue.\\n•Skills Used: Langchain, Llama model, Huggingface\\nEducation\\nIndian Institute of Science, Bangalore Aug. 2019 – July 2021\\nMaster of Technology in Computer Science\\nInstitute of Engineering and Technology, Lucknow Aug. 2015 – July 2019\\nBachelor of Technology in Information Technology\\nM.Tech. Thesis (GitHub Link)\\nTransformer Models for Assertion Generation in JAVA Unit Tests Feb. 2020 – June 2021\\nGuide: Prof. Aditya Kanade (SEAL Lab) IISc. Bangalore, Karnataka\\n•Automatic Assert Statement Generation for Java Unit Test Cases using multiple Transformers Models Fused with Pretrained\\nCuBERT ( CodeUnderstanding BERT ) Encoder - BERT Large Transformer Encoder by Google.\\n•Fed the JUnit Test methods as input to the CuBERT encoder and used it’s output as sentence embeddings for the Junit Test\\nmethod. Fed this output as input to the modified Tranformer models.\\n•Used drop-net technique to outperform the baseline models.' metadata={'source': 'C:\\\\Users\\\\SACHENDRA\\\\Documents\\\\Activeloop\\\\Langchain_and_Vector_Databases_in_Production\\\\4_Keeping_knowledge_organized_with_indexes\\\\3_What_are_Text_Splitter_and_why_are_they_useful\\\\Sachendra_CV.pdf', 'page': 0}\n",
      "You have 2 documents\n",
      "Preview:\n",
      "Sachendra Chandra\n",
      "♂phone6394633084, 9410411981 /envel⌢pe07sachendra@gmail.com /linkedinLinkedIn /githubGitHub\n",
      "Work Experience\n",
      "Data Engineeer 2, Rakuten India August 2021 – Present\n",
      "Mart Migration\n",
      "•Part of data platform migration project from Teradata data to Bigquery. Converted the Systemwalker Jobnet into\n",
      "Airflow Python Scripts. These jobnets helped in creating the data marts in Teradata. The corresponding Airflow scripts\n",
      "orchestrated the same data mart creation in BigQuery.\n",
      "•Analyzed the translated Teradata SQL to BigQuery SQLs and the data marts created in BigQuery to ensure exact match\n",
      "between the data of Teradata that with BigQuery and automated the validation process for translated queries on Google\n",
      "BigQuery and Hive and involed in data ingestion from Teradata to BigQuery via Jenkins.\n",
      "•Part of migration task from C5000 to Minio and Bigquery. Translated the Airflow scripts for daily ingestion from Hive to\n",
      "Minio and BigQuery.\n",
      "•Skills Used: Python, SQL, Apache Airflow, BigQuery, Teradata, HiveQL\n",
      "POC of Data Mgmt. Tool like Spline and DataHub\n",
      "•Integrated Spline Data Lineage tool with existing code base to track Lineage of Spark job used for mart creation in\n",
      "BigQuery and Minio.\n",
      "•Worked on creating a Custom dispatcher using the spark spline agent code base so as to extract relevant lineage\n",
      "information for Spark mart creation jobs.\n",
      "•Explored DataHub for capturing, tracking, and visualizing data lineage, allowing organizations to gain valuable insights\n",
      "into their data ecosystems.\n",
      "•Skills Used: Python, Spline, DataHub, Apache Spark\n",
      "Internal Metadata Management\n",
      "•Worked on building framework to capture the dataset and datasource lineage and dataset metadata for spark job used\n",
      "for mart building.\n",
      "•Developed REST Client SDK that leverages Hive Metastore service for capturing matadata information of spark job and\n",
      "dispatching Metadata and lineage information to API layer.\n",
      "•Skills Used: Apache Spark, Hive, ArangoDB, Docker\n",
      "Operational Artificial General Intelligence (OAGI)\n",
      "•Thousands of jobs running every day and they consume cores, memories, storages and network bandwidth to full-fill\n",
      "their functional goals; those cores, memories, storages and network bandwidth are having the cost based on usage; if\n",
      "those processes are unnecessarily retried and rerun in a circumstance where errors are non recoverable, business has to\n",
      "pay unnecessary cost for those resources consumed by processes.\n",
      "•Before retrying, the process asks for the opinion from the OAGI service which is powered by LLM(Large Language\n",
      "Model) which has given the enough knowledge of what kind of exceptions/errors are re-triable and non re-triable. This\n",
      "saves many resources and directly impacts business revenue.\n",
      "•Skills Used: Langchain, Llama model, Huggingface\n",
      "Education\n",
      "Indian Institute of Science, Bangalore Aug. 2019 – July 2021\n",
      "Master of Technology in Computer Science\n",
      "Institute of Engineering and Technology, Lucknow Aug. 2015 – July 2019\n",
      "Bachelor of Technology in Information Technology\n",
      "M.Tech. Thesis (GitHub Link)\n",
      "Transformer Models for Assertion Generation in JAVA Unit Tests Feb. 2020 – June 2021\n",
      "Guide: Prof. Aditya Kanade (SEAL Lab) IISc. Bangalore, Karnataka\n",
      "•Automatic Assert Statement Generation for Java Unit Test Cases using multiple Transformers Models Fused with Pretrained\n",
      "CuBERT ( CodeUnderstanding BERT ) Encoder - BERT Large Transformer Encoder by Google.\n",
      "•Fed the JUnit Test methods as input to the CuBERT encoder and used it’s output as sentence embeddings for the Junit Test\n",
      "method. Fed this output as input to the modified Tranformer models.\n",
      "•Used drop-net technique to outperform the baseline models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(texts[0])\n",
    "\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "print (\"Preview:\")\n",
    "print (texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991d15f-034b-4c40-a403-38934632de62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
